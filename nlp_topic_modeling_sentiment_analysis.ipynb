{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poomkusa/Artificial-Intelligence-and-the-Future-of-Marketing-Education-Special-Issue/blob/main/nlp_topic_modeling_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP prediction and topic modeling"
      ],
      "metadata": {
        "id": "i03vbl476Gu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will take alook at text data. We will do:\n",
        "\n",
        "    1) Topic Modeling\n",
        "    2) Classification for title that are rated as 'over 18'"
      ],
      "metadata": {
        "id": "cCcwPoXU6GvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.style as style\n",
        "style.use('fivethirtyeight')\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from numpy import interp\n",
        "from itertools import cycle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "h051zFn-6GvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "9quq6wXr7RPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "\n",
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['cleaned_dat.csv']))"
      ],
      "metadata": {
        "_cell_guid": "",
        "_uuid": "",
        "trusted": true,
        "id": "dGFkNKef6GvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KPJvjIqT6GvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "noq-Gms36GvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data.isnull().sum() / len(data)) * 100"
      ],
      "metadata": {
        "trusted": true,
        "id": "S-kjCwJz6GvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see some columns having too many missing values. We will drop those."
      ],
      "metadata": {
        "id": "WJT3p5jz6GvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del data['id']\n",
        "del data['author_flair_text']\n",
        "del data['removed_by']\n",
        "del data['total_awards_received']\n",
        "del data['awarders']\n",
        "del data['created_utc']\n",
        "del data['full_link']"
      ],
      "metadata": {
        "trusted": true,
        "id": "OARtzHGb6GvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()"
      ],
      "metadata": {
        "trusted": true,
        "id": "obtbg9It6GvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "M2Zo9Vwn6GvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "LSHPxMn06GvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have only the relevant information and no missing values."
      ],
      "metadata": {
        "id": "oyUPNDqi6GvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check out numeric columns\n",
        "\n",
        "data.describe().T"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ra3vyD456GvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both columns look like they are heavily skewed when comparing the 75 percentile and max values."
      ],
      "metadata": {
        "id": "3Zn-kMx76GvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IdYvM8Sg6GvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The score distribution is heavily skewed"
      ],
      "metadata": {
        "id": "KkmhGvBG6GvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,5))\n",
        "\n",
        "sns.kdeplot(data['score'], shade=  True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TATnoVnO6GvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data[data['score'] < 10]), 'Posts with less than 10 votes')\n",
        "print(len(data[data['score'] > 10]), 'Posts with more than 10 votes')"
      ],
      "metadata": {
        "trusted": true,
        "id": "lOMcEYmk6GvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Same with the comments"
      ],
      "metadata": {
        "id": "TiKrx9IJ6GvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,5))\n",
        "\n",
        "sns.kdeplot(data['num_comments'], shade=  True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "R7263gOn6GvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data[data['num_comments'] < 10]), 'Posts with less than 10 comments')\n",
        "print(len(data[data['num_comments'] > 10]), 'Posts with more than 10 comments')"
      ],
      "metadata": {
        "trusted": true,
        "id": "tYf4nCx06GvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# post with the most comments\n",
        "\n",
        "data[data['score'] == data['score'].max()]['title'].iloc[0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "zrxMrgIv6GvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning"
      ],
      "metadata": {
        "id": "Exw0elnD6GvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are useful cleaning functions"
      ],
      "metadata": {
        "id": "AEd_bJJI6GvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def remove_line_breaks(text):\n",
        "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
        "    return text\n",
        "\n",
        "#remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n",
        "    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
        "    '''Escape all the characters in pattern except ASCII letters and numbers'''\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_zero_punctuation = []\n",
        "    for token in tokens:\n",
        "        if not re_replacements.match(token):\n",
        "            token = re_punctuation.sub(\" \", token)\n",
        "        tokens_zero_punctuation.append(token)\n",
        "    return ' '.join(tokens_zero_punctuation)\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def lowercase(text):\n",
        "    text_low = [token.lower() for token in word_tokenize(text)]\n",
        "    return ' '.join(text_low)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    text = \" \".join([word for word in word_tokens if word not in stop])\n",
        "    return text\n",
        "\n",
        "#remobe one character words\n",
        "def remove_one_character_words(text):\n",
        "    '''Remove words from dataset that contain only 1 character'''\n",
        "    text_high_use = [token for token in word_tokenize(text) if len(token)>1]\n",
        "    return ' '.join(text_high_use)\n",
        "\n",
        "#%%\n",
        "# Stemming with 'Snowball stemmer\" package\n",
        "def stem(text):\n",
        "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
        "    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]\n",
        "    return ' '.join(text_stemmed)\n",
        "\n",
        "def lemma(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])\n",
        "    return ' '.join(text_lemma)\n",
        "\n",
        "\n",
        "#break sentences to individual word list\n",
        "def sentence_word(text):\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    return word_tokens\n",
        "#break paragraphs to sentence token\n",
        "def paragraph_sentence(text):\n",
        "    sent_token = nltk.sent_tokenize(text)\n",
        "    return sent_token\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Return a list of words in a text.\"\"\"\n",
        "    return re.findall(r'\\w+', text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "    no_nums = re.sub(r'\\d+', '', text)\n",
        "    return ''.join(no_nums)\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    _steps = [\n",
        "    remove_line_breaks,\n",
        "    remove_one_character_words,\n",
        "    remove_special_characters,\n",
        "    lowercase,\n",
        "    remove_punctuation,\n",
        "    remove_stopwords,\n",
        "    stem,\n",
        "    remove_numbers\n",
        "]\n",
        "    for step in _steps:\n",
        "        text=step(text)\n",
        "    return text\n",
        "#%%\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_3-SXpxj6GvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using list comprehension we clean the titles and append the cleaned text as columns to the df."
      ],
      "metadata": {
        "id": "zuRiUe3x6GvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_title'] = pd.Series([clean_text(i) for i in tqdm(data['title'])])"
      ],
      "metadata": {
        "trusted": true,
        "id": "RR-RL1gh6GvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordcloud"
      ],
      "metadata": {
        "id": "VfH9DkEn6GvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = data[\"clean_title\"].values"
      ],
      "metadata": {
        "trusted": true,
        "id": "CwjehTRm6GvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls = []\n",
        "\n",
        "for i in words:\n",
        "    ls.append(str(i))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4EBBbVw36GvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls[:5]"
      ],
      "metadata": {
        "trusted": true,
        "id": "L4XUWaZL6GvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\n",
        "plt.figure(figsize=(16,13))\n",
        "wc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\n",
        "wc.generate(\" \".join(ls))\n",
        "plt.title(\"Most discussed terms\", fontsize=20)\n",
        "plt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "trusted": true,
        "id": "6Udj6K186GvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data, Visualization, USA and Time are popular terms.\n",
        "Original content seems to be very popular as well, as can be seen in the graph below."
      ],
      "metadata": {
        "id": "dY6eRdm66GvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Most popular posts"
      ],
      "metadata": {
        "id": "IwAICZ5j6GvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_pop = data.sort_values('score', ascending =False)[['title', 'score']].head(12)\n",
        "\n",
        "most_pop['score1'] = most_pop['score']/1000"
      ],
      "metadata": {
        "trusted": true,
        "id": "6Y8rYbaG6GvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,25))\n",
        "\n",
        "sns.barplot(data = most_pop, y = 'title', x = 'score1', color = 'c')\n",
        "plt.xticks(fontsize=27, rotation=0)\n",
        "plt.yticks(fontsize=31, rotation=0)\n",
        "plt.xlabel('Votes in Thousands', fontsize = 21)\n",
        "plt.ylabel('')\n",
        "plt.title('Most popular posts', fontsize = 30)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7-1lxCWr6GvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roughly 75% of the most popular titles are Original Content"
      ],
      "metadata": {
        "id": "ci08ZcXA6GvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kCw5X2UL6GvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_com = data.sort_values('num_comments', ascending =False)[['title', 'num_comments', 'author']].head(12)\n",
        "most_com['num_comments1'] = most_com['num_comments']/1000"
      ],
      "metadata": {
        "trusted": true,
        "id": "lV7H_7tE6GvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(most_com)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IkImJDex6GvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.reset_index()\n",
        "x[x['index'] == 92800]"
      ],
      "metadata": {
        "trusted": true,
        "id": "mK6Nbliy6GvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_com = most_com[most_com.author != 'dinoignacio']"
      ],
      "metadata": {
        "trusted": true,
        "id": "PnqRThHp6GvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,25))\n",
        "\n",
        "sns.barplot(data = most_com, y = 'title', x = 'num_comments1', color = 'y')\n",
        "plt.xticks(fontsize=28, rotation=0)\n",
        "plt.yticks(fontsize=30, rotation=0)\n",
        "plt.xlabel('Comments in Thousands', fontsize = 21)\n",
        "plt.ylabel('')\n",
        "plt.title('Most commented posts', fontsize = 30)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gGA50hni6GvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The titles with the most cpmments seem to be more controversial, which makes sense."
      ],
      "metadata": {
        "id": "dgVeTJre6GvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_com.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9tUI0seM6GvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = data.sort_values('score', ascending =False)\n",
        "\n",
        "n['score1'] = n['score']/1000\n",
        "n['num_comments1'] = n['num_comments']/1000"
      ],
      "metadata": {
        "trusted": true,
        "id": "BYcxmuOd6GvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,15))\n",
        "\n",
        "sns.regplot(data = n, y = 'score1', x = 'num_comments1', color = 'purple')\n",
        "plt.xticks(fontsize=14, rotation=0)\n",
        "plt.yticks(fontsize=14, rotation=0)\n",
        "plt.xlabel('Comments in Thousands', fontsize = 15)\n",
        "plt.ylabel('Votes in Thousands')\n",
        "plt.title('Comments and votes', fontsize = 14)"
      ],
      "metadata": {
        "trusted": true,
        "id": "r7yb78hF6GvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data['num_comments'] == data['num_comments'].max()]"
      ],
      "metadata": {
        "trusted": true,
        "id": "VGiTZrDB6GvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ],
      "metadata": {
        "id": "aHuUJ2np6GvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling"
      ],
      "metadata": {
        "id": "DIaKP4MF6GvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "PSxU4Ok56GvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "trusted": true,
        "id": "ecnWicKn6GvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "trusted": true,
        "id": "m762sLWw6GvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "metadata": {
        "trusted": true,
        "id": "ut0cF0lF6GvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['title'].iloc[1]"
      ],
      "metadata": {
        "trusted": true,
        "id": "LiGKV3786GvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_sample = data['title'].iloc[1]\n",
        "print('original document: ')\n",
        "\n",
        "words = []\n",
        "\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "\n",
        "\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(preprocess(doc_sample))"
      ],
      "metadata": {
        "trusted": true,
        "id": "WAgZASRL6Gva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "University to 'univers' --> not too good"
      ],
      "metadata": {
        "id": "tPCu97hU6Gvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "UUAqIeZ06Gvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_title'] = data['clean_title'].astype(str)"
      ],
      "metadata": {
        "trusted": true,
        "id": "28zhX_LD6Gvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "\n",
        "for i in data['clean_title']:\n",
        "        words.append(i.split(' '))"
      ],
      "metadata": {
        "trusted": true,
        "id": "VrEuKc7W6Gvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the dictionary\n",
        "\n",
        "\n",
        "\n",
        "--> every unique word in titles"
      ],
      "metadata": {
        "id": "RG0SHDQI6Gvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = gensim.corpora.Dictionary(words)\n",
        "\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "metadata": {
        "trusted": true,
        "id": "xuv-X2MJ6Gvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out tokens in the dictionary by their frequency.\n",
        "\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q9wRVlf_6Gvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Create Corpus -> term document frequency\n",
        "\n",
        "doc2bow() simply counts the number of occurrences of each distinct word,\n",
        "converts the word to its integer word ID and returns the result as a sparse vector."
      ],
      "metadata": {
        "id": "35lpZIiS6Gvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in words]\n",
        "bow_corpus[4310]"
      ],
      "metadata": {
        "trusted": true,
        "id": "wny4zzv86Gvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_doc_4310 = bow_corpus[4310]\n",
        "\n",
        "for i in range(len(bow_doc_4310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0],\n",
        "                                               dictionary[bow_doc_4310[i][0]],\n",
        "bow_doc_4310[i][1]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "QjnNrR2Q6Gvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF/IDF"
      ],
      "metadata": {
        "id": "gwLUqT9j6Gvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "id": "fnwAG-RC6Gvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
        "                                       num_topics=10,\n",
        "                                       id2word=dictionary,\n",
        "                                       passes=2,\n",
        "                                       workers=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pvt3fZol6Gvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we show the output of the model"
      ],
      "metadata": {
        "id": "uRW1miOi6Gvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "metadata": {
        "trusted": true,
        "id": "W53VMzec6Gvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parallelization uses multiprocessing; in case this doesn’t work for you for some reason, try the gensim.models.ldamodel.LdaModel class which is an equivalent, but more straightforward and single-core implementation."
      ],
      "metadata": {
        "id": "p7rwNEPX6Gvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n",
        "                                             num_topics=10,\n",
        "                                             id2word=dictionary,\n",
        "                                             passes=2,\n",
        "                                             workers=4)\n",
        "\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "metadata": {
        "trusted": true,
        "id": "AsosXebo6Gvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in the output, we should definitely also clean the text for links --> 'https:'"
      ],
      "metadata": {
        "id": "DCoRxEAX6Gvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets check out with how much certainty the model predicts a new title to belong to one of the created topics"
      ],
      "metadata": {
        "id": "zL_41Lz26Gvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "gZrSTgZa6Gvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict 'over_18' titles"
      ],
      "metadata": {
        "id": "JdkgmT5h6Gvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['over_18'] = data['over_18'].astype(int)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rpeWNShd6Gvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['over_18'] = pd.Categorical(data['over_18'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "0bMDFZ7M6Gvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check class balance"
      ],
      "metadata": {
        "id": "u9uBeBtg6Gvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(data['over_18'].value_counts(normalize=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "kgz_pgGi6Gvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am not sure if its possible to get reliable predictions with one class being only represented in 0.5% of the titles.\n",
        "\n",
        "I tried resampling, but the results at the end look way too good to be true."
      ],
      "metadata": {
        "id": "Szx4-fWq6Gvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample"
      ],
      "metadata": {
        "trusted": true,
        "id": "OHOEsN-T6Gvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = data[data.over_18==0]\n",
        "df_minority = data[data.over_18==1]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=180000) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "data_n = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Display new class counts\n",
        "data_n['over_18'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "sZl2r0XG6Gvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(data_n['over_18'].value_counts(normalize=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "bKM8MSAr6Gvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "trusted": true,
        "id": "lqZzOxSs6Gvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text = data_n['clean_title']"
      ],
      "metadata": {
        "trusted": true,
        "id": "WkoyVQNB6Gvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create TF/IDF again"
      ],
      "metadata": {
        "id": "eseVtQB16Gvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(processed_text)\n",
        "print(tfidf.shape)\n",
        "print('\\n')\n",
        "#print(vectorizer.get_feature_names())"
      ],
      "metadata": {
        "trusted": true,
        "id": "zuqoGSXe6Gvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_n['over_18']"
      ],
      "metadata": {
        "trusted": true,
        "id": "DWJfspVv6Gvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data_n['over_18']"
      ],
      "metadata": {
        "trusted": true,
        "id": "XmgvPegh6Gvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(tfidf, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wAzyL-Av6Gv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the training dataset on the NB classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(X_train_tf,y_train_tf)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB_tf = Naive.predict(X_test_tf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy -> \",accuracy_score(predictions_NB_tf, y_test_tf)*100)\n",
        "print(classification_report(predictions_NB_tf,y_test_tf))"
      ],
      "metadata": {
        "trusted": true,
        "id": "rm3ukg_K6Gv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logmodel = LogisticRegression()\n",
        "logmodel.fit(X_train_tf, y_train_tf)\n",
        "\n",
        "predictions_LR_tf = logmodel.predict(X_test_tf)\n",
        "\n",
        "print(\"LR Accuracy -> \",accuracy_score(predictions_LR_tf, y_test_tf)*100)\n",
        "print(classification_report(predictions_LR_tf,y_test_tf))"
      ],
      "metadata": {
        "trusted": true,
        "id": "aNBwtopr6Gv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}